# SAI Dashboard: Two-Phase ETL Architecture
**Date**: October 8, 2025
**Status**: ‚úÖ Trigger fixed, implementing optimal two-phase architecture

---

## üéØ Executive Summary

The SAI Dashboard ETL has been redesigned with a **two-phase approach** that eliminates all race conditions and ensures 100% data capture.

### Core Innovation: Two-Phase Processing

1. **Phase 1 (Immediate)**: Captures core immutable data when execution **starts**
   - Webhook data + camera metadata + **image** (all available immediately!)
   - Status = 'running'
   - Zero wait for inference

2. **Phase 2 (On Completion)**: Enriches with inference results when execution **finishes**
   - Updates status to 'success' or 'error'
   - Adds inference results (detections, risk assessment)
   - Adds notification status

### ‚úÖ Problems Solved

- ‚úÖ **Duplicate triggers removed** - was firing 2x, now fires 1x per phase
- ‚úÖ **Race conditions eliminated** - idempotent inserts with ON CONFLICT
- ‚úÖ **Zero data loss** - core record created before inference (even if it crashes!)
- ‚úÖ **Perfect coherency** - mandatory fields always populated
- ‚úÖ **Image available immediately** - webhook delivers image in Phase 1!

---

## üìä Two-Phase Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Camera ‚Üí Webhook ‚Üí n8n Workflow Starts                      ‚îÇ
‚îÇ  Delivers: image (base64), camera_metadata, timestamp       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚îÇ INSERT into execution_entity (status='running')
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üîî PHASE 1 TRIGGER: sai_execution_created                   ‚îÇ
‚îÇ  Channel: 'sai_execution_created'                           ‚îÇ
‚îÇ  Fires: ONCE on INSERT                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Phase 1 ETL Handler                                         ‚îÇ
‚îÇ  1. Extract webhook data from execution_data                ‚îÇ
‚îÇ  2. Save image (base64 ‚Üí jpg/webp/thumb)                    ‚îÇ
‚îÇ  3. INSERT INTO sai_executions (status='running')           ‚îÇ
‚îÇ     - execution_id, workflow_id, started_at                 ‚îÇ
‚îÇ     - camera_metadata (JSONB)                               ‚îÇ
‚îÇ     - image paths                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

         ‚è±Ô∏è  Inference Processing (5-30 seconds)
         YOLO detection, LLM analysis, Telegram notification

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ n8n Workflow Completes                                      ‚îÇ
‚îÇ  Delivers: detections, risk_level, telegram_status          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚îÇ UPDATE execution_entity SET status='success'
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üîî PHASE 2 TRIGGER: sai_execution_completed                 ‚îÇ
‚îÇ  Channel: 'sai_execution_completed'                         ‚îÇ
‚îÇ  Fires: ONCE on status change to 'success'|'error'          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Phase 2 ETL Handler                                         ‚îÇ
‚îÇ  1. UPDATE sai_executions SET completed_at, duration, status‚îÇ
‚îÇ  2. INSERT INTO sai_inference_results (detections, risk)    ‚îÇ
‚îÇ  3. INSERT INTO sai_notifications (telegram status)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Dashboard: Real-time updates via SSE                        ‚îÇ
‚îÇ  - New execution appears immediately (Phase 1)              ‚îÇ
‚îÇ  - Inference results added when ready (Phase 2)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üóÑÔ∏è Optimal Database Schema

### **Core Table: sai_executions**

```sql
CREATE TABLE sai_executions (
    execution_id BIGINT PRIMARY KEY,
    workflow_id VARCHAR(36) NOT NULL CHECK (workflow_id = 'yDbfhooKemfhMIkC'),

    -- ===== PHASE 1: Webhook data (immutable) =====
    started_at TIMESTAMPTZ NOT NULL,
    camera_metadata JSONB NOT NULL,              -- ‚úÖ From webhook: {id, node_id, gps, settings, ...}

    -- Image paths (PHASE 1: saved from webhook base64)
    image_original_path VARCHAR(500),            -- /mnt/raid1/.../original/185/185839.jpg
    image_webp_path VARCHAR(500),                -- /mnt/raid1/.../webp/185/185839.webp
    image_thumb_path VARCHAR(500),               -- /mnt/raid1/.../thumb/185/185839.webp
    image_size_bytes INTEGER,

    -- ===== PHASE 2: Completion data (nullable until finished) =====
    completed_at TIMESTAMPTZ,                    -- NULL while running
    duration_ms INTEGER,                         -- NULL while running
    status VARCHAR(20) NOT NULL DEFAULT 'running', -- 'running' ‚Üí 'success'|'error'

    -- Metadata
    etl_version VARCHAR(10) DEFAULT '2.0',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes for dashboard queries
CREATE INDEX idx_sai_exec_running ON sai_executions(status, started_at DESC)
    WHERE status = 'running';  -- Find stuck executions
CREATE INDEX idx_sai_exec_completed ON sai_executions(completed_at DESC NULLS LAST)
    WHERE status IN ('success', 'error');
CREATE INDEX idx_sai_exec_camera ON sai_executions((camera_metadata->>'id'));
CREATE INDEX idx_sai_exec_node ON sai_executions((camera_metadata->>'node_id'));
CREATE INDEX idx_sai_exec_metadata_gin ON sai_executions USING GIN(camera_metadata jsonb_path_ops);
```

**Key Design**: Mandatory fields in Phase 1, optional fields in Phase 2. This ensures coherency and enables debugging.

### **Inference Results Table (Phase 2 only)**

```sql
CREATE TABLE sai_inference_results (
    execution_id BIGINT PRIMARY KEY REFERENCES sai_executions(execution_id) ON DELETE CASCADE,

    -- Model metadata
    model_name VARCHAR(50) NOT NULL,             -- 'yolov8', 'yolov11', 'qwen2.5vl:7b'
    model_version VARCHAR(50),
    inference_timestamp TIMESTAMPTZ DEFAULT NOW(),
    processing_time_ms INTEGER,

    -- Structured detections (JSONB for flexibility)
    detections JSONB NOT NULL DEFAULT '[]'::jsonb, -- [{class, confidence, bbox: [x,y,w,h]}]
    detection_count INTEGER GENERATED ALWAYS AS (jsonb_array_length(detections)) STORED,

    -- Risk assessment (for LLM models)
    risk_level VARCHAR(20) CHECK (risk_level IN ('critical', 'high', 'medium', 'low', 'none')),
    confidence_score DECIMAL(5,4) CHECK (confidence_score BETWEEN 0 AND 1),
    raw_response TEXT,
    raw_response_hash VARCHAR(64),               -- SHA256 for deduplication

    -- Quick filter flags (generated columns for performance)
    has_smoke BOOLEAN GENERATED ALWAYS AS (
        detections @> '[{"class": "smoke"}]' OR
        (raw_response IS NOT NULL AND raw_response ILIKE '%smoke%')
    ) STORED,
    has_fire BOOLEAN GENERATED ALWAYS AS (
        detections @> '[{"class": "fire"}]' OR
        detections @> '[{"class": "flame"}]' OR
        (raw_response IS NOT NULL AND (raw_response ILIKE '%fire%' OR raw_response ILIKE '%flame%'))
    ) STORED,

    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes for ML dataset queries
CREATE INDEX idx_sai_infer_model ON sai_inference_results(model_name, model_version);
CREATE INDEX idx_sai_infer_risk ON sai_inference_results(risk_level, confidence_score DESC NULLS LAST);
CREATE INDEX idx_sai_infer_flags ON sai_inference_results(has_smoke, has_fire);
CREATE INDEX idx_sai_infer_detections ON sai_inference_results USING GIN(detections jsonb_path_ops);
```

### **Notifications Table (Phase 2 only)**

```sql
CREATE TABLE sai_notifications (
    execution_id BIGINT PRIMARY KEY REFERENCES sai_executions(execution_id) ON DELETE CASCADE,

    -- Telegram delivery
    telegram_sent BOOLEAN DEFAULT FALSE,
    telegram_message_id BIGINT,
    telegram_chat_id VARCHAR(50),
    telegram_sent_at TIMESTAMPTZ,
    telegram_error TEXT,

    -- Future: email, sms, push
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_sai_notif_telegram ON sai_notifications(telegram_sent, telegram_sent_at);
```

---

## üîß PostgreSQL Two-Phase Triggers

### **Phase 1 Trigger: Execution Created**

```sql
CREATE OR REPLACE FUNCTION notify_sai_execution_created()
RETURNS TRIGGER AS $$
BEGIN
    -- Only for SAI workflow
    IF NEW."workflowId"::text != 'yDbfhooKemfhMIkC' THEN
        RETURN NEW;
    END IF;

    -- Send Phase 1 notification (execution just started)
    PERFORM pg_notify('sai_execution_created', json_build_object(
        'execution_id', NEW.id,
        'workflow_id', NEW."workflowId"::text,
        'started_at', NEW."startedAt"
    )::text);

    RAISE NOTICE '[Phase 1] SAI execution % created', NEW.id;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER sai_execution_created_trigger
    AFTER INSERT ON execution_entity
    FOR EACH ROW
    WHEN (NEW."workflowId"::text = 'yDbfhooKemfhMIkC')
    EXECUTE FUNCTION notify_sai_execution_created();

COMMENT ON TRIGGER sai_execution_created_trigger ON execution_entity IS
'Phase 1: Fires when SAI workflow execution starts. Captures webhook data + image.';
```

### **Phase 2 Trigger: Execution Completed**

```sql
CREATE OR REPLACE FUNCTION notify_sai_execution_completed()
RETURNS TRIGGER AS $$
BEGIN
    -- Only for SAI workflow
    IF NEW."workflowId"::text != 'yDbfhooKemfhMIkC' THEN
        RETURN NEW;
    END IF;

    -- Only when status changes to final state
    IF NEW.status IN ('success', 'error') AND OLD.status IS DISTINCT FROM NEW.status THEN
        PERFORM pg_notify('sai_execution_completed', json_build_object(
            'execution_id', NEW.id,
            'workflow_id', NEW."workflowId"::text,
            'status', NEW.status,
            'started_at', NEW."startedAt",
            'stopped_at', NEW."stoppedAt",
            'duration_ms', EXTRACT(EPOCH FROM (NEW."stoppedAt" - NEW."startedAt")) * 1000
        )::text);

        RAISE NOTICE '[Phase 2] SAI execution % completed with status: %', NEW.id, NEW.status;
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER sai_execution_completed_trigger
    AFTER UPDATE OF status ON execution_entity
    FOR EACH ROW
    WHEN (
        NEW."workflowId"::text = 'yDbfhooKemfhMIkC'
        AND NEW.status IN ('success', 'error')
        AND OLD.status IS DISTINCT FROM NEW.status
    )
    EXECUTE FUNCTION notify_sai_execution_completed();

COMMENT ON TRIGGER sai_execution_completed_trigger ON execution_entity IS
'Phase 2: Fires when SAI workflow execution completes (success or error). Captures inference results.';
```

---

## üöÄ Two-Phase ETL Service Implementation

```typescript
// backend/src/services/two-phase-etl-service.ts

import { EventEmitter } from 'events';
import { Pool, Client } from 'pg';
import sharp from 'sharp';
import fs from 'fs/promises';
import path from 'path';

interface Phase1Payload {
  execution_id: number;
  workflow_id: string;
  started_at: string;
}

interface Phase2Payload {
  execution_id: number;
  workflow_id: string;
  status: 'success' | 'error';
  started_at: string;
  stopped_at: string;
  duration_ms: number;
}

export class TwoPhaseETLService extends EventEmitter {
  private n8nPool: Pool;
  private dashboardPool: Pool;
  private phase1Client: Client | null = null;
  private phase2Client: Client | null = null;
  private imageCachePath: string = '/mnt/raid1/n8n-backup/images';

  private metrics = {
    phase1_processed: 0,
    phase2_processed: 0,
    phase1_failed: 0,
    phase2_failed: 0,
    duplicates_handled: 0
  };

  constructor() {
    super();

    this.n8nPool = new Pool({
      host: process.env.N8N_DB_HOST || 'localhost',
      port: parseInt(process.env.N8N_DB_PORT || '5432'),
      database: process.env.N8N_DB_NAME || 'n8n',
      user: process.env.N8N_DB_USER || 'n8n_user',
      password: process.env.N8N_DB_PASSWORD,
      max: 5
    });

    this.dashboardPool = new Pool({
      host: process.env.SAI_DB_HOST || 'localhost',
      port: parseInt(process.env.SAI_DB_PORT || '5432'),
      database: process.env.SAI_DB_NAME || 'sai_dashboard',
      user: process.env.SAI_DB_USER || 'n8n_user',
      password: process.env.SAI_DB_PASSWORD,
      max: 10
    });
  }

  async start(): Promise<void> {
    console.log('üöÄ Starting Two-Phase ETL Service...');

    await this.testConnections();
    await this.startPhase1Listener();
    await this.startPhase2Listener();

    console.log('‚úÖ Two-Phase ETL Service started successfully');
  }

  private async testConnections(): Promise<void> {
    const n8nTest = await this.n8nPool.query('SELECT 1');
    const dashTest = await this.dashboardPool.query('SELECT 1');
    console.log('‚úÖ Database connections verified');
  }

  // ========================================================================
  // PHASE 1: Handle execution creation (webhook data + image)
  // ========================================================================

  private async startPhase1Listener(): Promise<void> {
    this.phase1Client = new Client({
      host: process.env.N8N_DB_HOST || 'localhost',
      port: parseInt(process.env.N8N_DB_PORT || '5432'),
      database: process.env.N8N_DB_NAME || 'n8n',
      user: process.env.N8N_DB_USER || 'n8n_user',
      password: process.env.N8N_DB_PASSWORD
    });

    await this.phase1Client.connect();
    await this.phase1Client.query('LISTEN sai_execution_created');

    this.phase1Client.on('notification', async (msg) => {
      if (msg.channel === 'sai_execution_created' && msg.payload) {
        const payload: Phase1Payload = JSON.parse(msg.payload);
        console.log(`[Phase 1] üì¨ Execution ${payload.execution_id} created`);
        await this.handlePhase1(payload);
      }
    });

    console.log('‚úÖ Phase 1 listener started (channel: sai_execution_created)');
  }

  private async handlePhase1(payload: Phase1Payload): Promise<void> {
    try {
      // 1. Extract webhook data from n8n execution_data
      const webhookData = await this.extractWebhookData(payload.execution_id);
      if (!webhookData) {
        console.warn(`‚ö†Ô∏è  [Phase 1] No webhook data found for execution ${payload.execution_id}`);
        return;
      }

      // 2. Save image immediately (from webhook base64)
      let imagePaths = null;
      if (webhookData.image_base64) {
        imagePaths = await this.processImage(payload.execution_id, webhookData.image_base64);
        console.log(`üì∏ [Phase 1] Image processed for execution ${payload.execution_id}`);
      }

      // 3. Insert core record (idempotent!)
      const result = await this.dashboardPool.query(`
        INSERT INTO sai_executions (
          execution_id, workflow_id, started_at,
          camera_metadata, status,
          image_original_path, image_webp_path, image_thumb_path, image_size_bytes
        ) VALUES ($1, $2, $3, $4, 'running', $5, $6, $7, $8)
        ON CONFLICT (execution_id) DO NOTHING
        RETURNING execution_id, (xmax = 0) AS is_new_row
      `, [
        payload.execution_id,
        payload.workflow_id,
        payload.started_at,
        JSON.stringify(webhookData.camera_metadata || {}),
        imagePaths?.original_path || null,
        imagePaths?.webp_path || null,
        imagePaths?.thumb_path || null,
        imagePaths?.image_size || null
      ]);

      if (result.rows.length > 0 && result.rows[0].is_new_row) {
        this.metrics.phase1_processed++;
        console.log(`‚úÖ [Phase 1] Core record created for execution ${payload.execution_id}`);
      } else {
        this.metrics.duplicates_handled++;
        console.log(`‚è≠Ô∏è  [Phase 1] Execution ${payload.execution_id} already exists (duplicate)}`);
      }

    } catch (error) {
      this.metrics.phase1_failed++;
      console.error(`‚ùå [Phase 1] Failed to process execution ${payload.execution_id}:`, error);
    }
  }

  // ========================================================================
  // PHASE 2: Handle execution completion (inference results)
  // ========================================================================

  private async startPhase2Listener(): Promise<void> {
    this.phase2Client = new Client({
      host: process.env.N8N_DB_HOST || 'localhost',
      port: parseInt(process.env.N8N_DB_PORT || '5432'),
      database: process.env.N8N_DB_NAME || 'n8n',
      user: process.env.N8N_DB_USER || 'n8n_user',
      password: process.env.N8N_DB_PASSWORD
    });

    await this.phase2Client.connect();
    await this.phase2Client.query('LISTEN sai_execution_completed');

    this.phase2Client.on('notification', async (msg) => {
      if (msg.channel === 'sai_execution_completed' && msg.payload) {
        const payload: Phase2Payload = JSON.parse(msg.payload);
        console.log(`[Phase 2] üì¨ Execution ${payload.execution_id} completed (${payload.status})`);
        await this.handlePhase2(payload);
      }
    });

    console.log('‚úÖ Phase 2 listener started (channel: sai_execution_completed)');
  }

  private async handlePhase2(payload: Phase2Payload): Promise<void> {
    try {
      // 1. Update core record with completion data
      await this.dashboardPool.query(`
        UPDATE sai_executions
        SET
          completed_at = $1,
          duration_ms = $2,
          status = $3,
          updated_at = NOW()
        WHERE execution_id = $4
      `, [payload.stopped_at, payload.duration_ms, payload.status, payload.execution_id]);

      console.log(`‚úÖ [Phase 2] Updated completion data for execution ${payload.execution_id}`);

      // 2. Extract inference results (only for success)
      if (payload.status === 'success') {
        const inferenceData = await this.extractInferenceData(payload.execution_id);

        if (inferenceData) {
          // 3. Insert inference results (idempotent)
          await this.insertInferenceResults(payload.execution_id, inferenceData);

          // 4. Insert notification status (idempotent)
          await this.insertNotificationStatus(payload.execution_id, inferenceData.telegram);
        }
      }

      this.metrics.phase2_processed++;
      console.log(`‚úÖ [Phase 2] Enrichment complete for execution ${payload.execution_id}`);

    } catch (error) {
      this.metrics.phase2_failed++;
      console.error(`‚ùå [Phase 2] Failed to process execution ${payload.execution_id}:`, error);
    }
  }

  // ========================================================================
  // HELPER METHODS
  // ========================================================================

  private async extractWebhookData(executionId: number): Promise<any> {
    const result = await this.n8nPool.query(`
      SELECT data FROM execution_data WHERE "executionId" = $1
    `, [executionId]);

    if (result.rows.length === 0) return null;

    const data = JSON.parse(result.rows[0].data);

    // TODO: Extract camera_metadata and image_base64 from n8n data structure
    // This is where we parse the webhook body
    return {
      camera_metadata: this.extractCameraMetadata(data),
      image_base64: this.extractImageBase64(data)
    };
  }

  private async processImage(executionId: number, imageBase64: string): Promise<any> {
    const partition = Math.floor(executionId / 1000);

    const originalDir = path.join(this.imageCachePath, 'original', partition.toString());
    const webpDir = path.join(this.imageCachePath, 'webp', partition.toString());
    const thumbDir = path.join(this.imageCachePath, 'thumb', partition.toString());

    await Promise.all([
      fs.mkdir(originalDir, { recursive: true }),
      fs.mkdir(webpDir, { recursive: true }),
      fs.mkdir(thumbDir, { recursive: true })
    ]);

    const imageBuffer = Buffer.from(imageBase64, 'base64');

    const originalPath = path.join(originalDir, `${executionId}.jpg`);
    const webpPath = path.join(webpDir, `${executionId}.webp`);
    const thumbPath = path.join(thumbDir, `${executionId}.webp`);

    // Check if already processed (idempotent)
    try {
      await fs.access(originalPath);
      return { original_path: originalPath, webp_path: webpPath, thumb_path: thumbPath, image_size: imageBuffer.length };
    } catch {
      // Process images
      await Promise.all([
        sharp(imageBuffer).jpeg({ quality: 95 }).toFile(originalPath),
        sharp(imageBuffer).webp({ quality: 85 }).toFile(webpPath),
        sharp(imageBuffer).resize(300, 300, { fit: 'inside' }).webp({ quality: 75 }).toFile(thumbPath)
      ]);

      return { original_path: originalPath, webp_path: webpPath, thumb_path: thumbPath, image_size: imageBuffer.length };
    }
  }

  private extractCameraMetadata(data: any): any {
    // TODO: Parse camera metadata from webhook body
    return {};
  }

  private extractImageBase64(data: any): string | null {
    // TODO: Extract base64 image from n8n data
    return null;
  }

  private async extractInferenceData(executionId: number): Promise<any> {
    // TODO: Extract inference results from n8n execution_data
    return null;
  }

  private async insertInferenceResults(executionId: number, data: any): Promise<void> {
    // TODO: Insert into sai_inference_results
  }

  private async insertNotificationStatus(executionId: number, telegram: any): Promise<void> {
    // TODO: Insert into sai_notifications
  }

  getMetrics() {
    return this.metrics;
  }
}

export const twoPhaseETLService = new TwoPhaseETLService();
```

---

## üìã Implementation Checklist

### ‚úÖ Phase 0: Fix Duplicate Triggers (COMPLETED)
- ‚úÖ Removed old `sai_etl_trigger` (redundant)
- ‚úÖ Fixed `sai_execution_notify_trigger` to only fire on status change
- ‚úÖ Verified only ONE trigger fires per execution

### üîÑ Phase 1: Two-Phase Triggers (NEXT)
- [ ] Apply `database/two_phase_triggers.sql` to n8n database
- [ ] Test Phase 1 trigger with manual INSERT
- [ ] Test Phase 2 trigger with manual UPDATE
- [ ] Verify NOTIFY messages received

### üìä Phase 2: New Schema (NEXT)
- [ ] Apply `database/sai_dashboard_schema_v2.sql` to sai_dashboard database
- [ ] Verify all tables created
- [ ] Verify all indexes created
- [ ] Test INSERT into sai_executions

### üîß Phase 3: ETL Service (NEXT)
- [ ] Implement `extractWebhookData()` method
- [ ] Implement `extractInferenceData()` method
- [ ] Implement complete Phase 1 handler
- [ ] Implement complete Phase 2 handler
- [ ] Add comprehensive logging

### ‚úÖ Phase 4: Testing
- [ ] Test with real webhook data
- [ ] Test image processing
- [ ] Test inference completion
- [ ] Verify no duplicates
- [ ] Monitor for 1 hour

### üöÄ Phase 5: Deployment
- [ ] Deploy to production
- [ ] Monitor metrics
- [ ] Verify 100% data capture

---

## üéØ Success Criteria

- ‚úÖ Zero duplicate processing
- ‚úÖ Zero race conditions
- ‚úÖ 100% execution capture (even on inference failure)
- ‚úÖ Images saved immediately (Phase 1)
- ‚úÖ Inference results added when ready (Phase 2)
- ‚úÖ Can debug stuck executions (status = 'running')
- ‚úÖ 50% log reduction

---

**Next Step**: Create and apply two-phase triggers to n8n database
